{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GV5jjLTTAyqm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class GridWorld:\n",
    "    # call to initialize grid world enviroment, default will be a 5x5 grid with a start point at the top left corner and goal at\n",
    "    # the bottom right corner, no obsticals, if probablity of obsticals is given, these will be placed in enviroment randomly\n",
    "    def __init__(self, grid_size=(5, 5), start_point=(0, 0), goal_point=None, prob_obstacle=0):\n",
    "        self.grid_size = grid_size\n",
    "        self.start_point = start_point\n",
    "        self.goal_point = goal_point if goal_point is not None else (grid_size[0] - 1, grid_size[1] - 1)\n",
    "        self.prob_obstacle = prob_obstacle\n",
    "\n",
    "        self.action_map = ['up', 'down', 'left', 'right']\n",
    "        self.grid = np.zeros(self.grid_size, dtype=int)\n",
    "        self._generate_obstacles()\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    # obstacles created based on probability argument passed\n",
    "    # idealy a small number (> .8) is passed so path to goal is still achivable\n",
    "    def _generate_obstacles(self):\n",
    "        self.grid = np.zeros(self.grid_size, dtype=int)\n",
    "        for r in range(self.grid_size[0]):\n",
    "            for c in range(self.grid_size[1]):\n",
    "                if random.random() < self.prob_obstacle and (r, c) != self.start_point and (r, c) != self.goal_point:\n",
    "                    self.grid[r, c] = 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_point = self.start_point\n",
    "\n",
    "        return self.agent_point\n",
    "\n",
    "    def step(self, action_idx):\n",
    "        action = self.action_map[action_idx]\n",
    "        x, y = self.agent_point\n",
    "        next_x, next_y = x, y\n",
    "\n",
    "        if action == 'up' and x > 0:\n",
    "            next_x -= 1\n",
    "        elif action == 'down' and x < self.grid_size[0] - 1:\n",
    "            next_x += 1\n",
    "        elif action == 'left' and y > 0:\n",
    "            next_y -= 1\n",
    "        elif action == 'right' and y < self.grid_size[1] - 1:\n",
    "            next_y += 1\n",
    "\n",
    "        is_obstacle = self.grid[next_x, next_y] == 1\n",
    "\n",
    "        if is_obstacle:                              # Penalty applied if agent bumps into obstacle\n",
    "            next_x, next_y = x, y\n",
    "            reward = -5\n",
    "            done = False\n",
    "        else:\n",
    "            self.agent_point = (next_x, next_y)     # For non obstacle states, reward ig goal, otherwise penalize\n",
    "            done = self.is_at_goal()\n",
    "            reward = 1000 if done else -1\n",
    "\n",
    "        return self.agent_point, reward, done, {}\n",
    "\n",
    "    def is_at_goal(self):\n",
    "        return self.agent_point == self.goal_point\n",
    "\n",
    "    # For debugging\n",
    "    def display_env(self):\n",
    "        world = np.array(self.grid, dtype=object)\n",
    "        x, y = self.agent_point\n",
    "        gx, gy = self.goal_point\n",
    "        world[x, y] = 'A'          # Agent\n",
    "        world[gx, gy] = 'G'        # Goal\n",
    "        print(world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cbmSyMLEBBFo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, grid_size, action_size=4, alpha=0.1, gamma=0, epsilon=0, epsilon_min=0.01, epsilon_decay=0):\n",
    "\n",
    "        self.grid_size = grid_size                # Size of the grid (grid_size x grid_size)\n",
    "        self.action_size = action_size            # Size of the action space (default 4: up, right, down, left)\n",
    "        self.alpha = alpha                        # Learning rate\n",
    "        self.gamma = gamma                        # Discount factor\n",
    "        self.epsilon = epsilon                    # Initial exploration rate\n",
    "        self.epsilon_min = epsilon_min            # Minimum exploration rate, allows for agent to continue exploring\n",
    "        self.epsilon_decay = epsilon_decay        # Decay rate for exploration\n",
    "\n",
    "        # Initialize Q-table with zeros\n",
    "        # Q-table shape: (grid_size, grid_size, action_size)\n",
    "        self.q_table = np.zeros((grid_size[0], grid_size[1], action_size))\n",
    "\n",
    "    def get_action(self, state, training=True):\n",
    "        # Get the current state, determine if exploration or exploitation\n",
    "        x, y = state\n",
    "        if training and np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.action_size)  # Exploration: choose a random action\n",
    "        else:\n",
    "            return np.argmax(self.q_table[x, y])        # Exploitation: choose the best action from Q-table\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        x, y = state                   # Current state\n",
    "        next_x, next_y = next_state\n",
    "\n",
    "        # Q-learning update rule\n",
    "        # Q(s,a) = Q(s,a) + alpha * [r + gamma * max(Q(s',a')) - Q(s,a)]\n",
    "\n",
    "        if not done:\n",
    "            target = reward + self.gamma * np.max(self.q_table[next_x, next_y])\n",
    "        else:\n",
    "            target = reward\n",
    "\n",
    "        current = self.q_table[x, y, action]\n",
    "        self.q_table[x, y, action] = current + self.alpha * (target - current)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        # Decay exploration rate\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "def train(env, agent, episodes=500, max_steps=100, log_interval=100):\n",
    "    rewards = []                     # Contains an array of reward totals accumulated in episodes\n",
    "    steps_list = []                  # Contains an array of step totals accumulated in episodes\n",
    "    q_changes = []                   # Contains an array of how the q table changed in between episodes\n",
    "    start_time = time.time()\n",
    "    total_reward_accumulated = 0\n",
    "\n",
    "    for episode in range(1, episodes + 1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        total_q_change = 0\n",
    "        done = False\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = agent.get_action(state)                         # Get action\n",
    "            old_q = agent.q_table[state[0], state[1], action]        # Get info for old Q table\n",
    "            next_state, reward, done, _ = env.step(action)           # Take step\n",
    "            agent.update(state, action, reward, next_state, done)    # Update Q-table\n",
    "\n",
    "            new_q = agent.q_table[state[0], state[1], action]        # Get info for new Q table\n",
    "            total_q_change += abs(new_q - old_q)                     # Q-value delta\n",
    "\n",
    "            # Update state and total reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Decay exploration rate\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "        # Save rewards, steps and q table change\n",
    "        rewards.append(total_reward)\n",
    "        total_reward_accumulated += total_reward\n",
    "        steps_list.append(step + 1)\n",
    "        q_changes.append(total_q_change)\n",
    "\n",
    "        # Log progress\n",
    "        if episode % log_interval == 0:\n",
    "            avg_reward = np.mean(rewards[-log_interval:])\n",
    "            avg_steps = np.mean(steps_list[-log_interval:])\n",
    "            avg_q_change = np.mean(q_changes[-log_interval:])\n",
    "            print(f\"Episode {episode}/{episodes} | Avg Reward: {avg_reward:.2f} | Avg Steps: {avg_steps:.2f} | QΔ: {avg_q_change:.4f} | Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "    print(f\"\\nTraining completed in {time.time() - start_time:.2f} seconds.\")\n",
    "    print(f\"\\nTotal Reward Accumulated: {total_reward_accumulated}.\\n\")\n",
    "    return rewards, q_changes\n",
    "\n",
    "def evaluate(env, agent, episodes=10, max_steps=100, render=True):\n",
    "      rewards = []                # Contains an array of reward totals accumulated in episodes\n",
    "      success_count = 0\n",
    "\n",
    "      for ep in range(episodes):\n",
    "          state = env.reset()\n",
    "          total_reward = 0\n",
    "          done = False   # clear for episode\n",
    "\n",
    "          path = [state]\n",
    "\n",
    "          # let agent find goal within max number of steps\n",
    "          for step in range(max_steps):\n",
    "              action = agent.get_action(state, training=False)\n",
    "              next_state, reward, done, _ = env.step(action)\n",
    "              state = next_state\n",
    "              path.append(state)\n",
    "              total_reward += reward\n",
    "              if done:\n",
    "                  if reward > 0: success_count += 1\n",
    "                  break\n",
    "\n",
    "          rewards.append(total_reward)\n",
    "\n",
    "          print(f\"Episode {ep+1}/{episodes} | Reward: {total_reward} | Steps: {step + 1}\")\n",
    "\n",
    "      print(f\"\\nSuccess Rate: {success_count / episodes * 100:.2f}%\\n\")\n",
    "\n",
    "      return rewards\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_convergence(rewards, q_changes):\n",
    "    # Plot Average Reward per Episode\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(rewards)\n",
    "    plt.title('Average Reward per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(q_changes)\n",
    "    plt.title(f'Average QΔ per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Q-value Change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-k3z2QiUBdFM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "\n",
    "# Network for DQ Agent\n",
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, grid_size, action_size=4, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, lr=1e-4, batch_size=64, memory_size=50000):\n",
    "        self.grid_size = grid_size                # Size of the grid (grid_size x grid_size)\n",
    "        self.state_size = 2                       # Size of state size (x, y)\n",
    "        self.action_size = action_size            # Size of the action space (default 4: up, right, down, left)\n",
    "        self.gamma = gamma                        # Discount factor\n",
    "        self.epsilon = epsilon                    # Initial exploration rate\n",
    "        self.epsilon_min = epsilon_min            # Minimum exploration rate, allows for agent to continue exploring\n",
    "        self.epsilon_decay = epsilon_decay        # Decay rate for exploration\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "\n",
    "        self.model = DQNetwork(self.state_size, action_size)\n",
    "        self.target_model = DQNetwork(self.state_size, self.action_size)  # Target network\n",
    "        self.target_model.load_state_dict(self.model.state_dict())  # Initialize target model with the same weights\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def update_target_network(self):\n",
    "        # Update the target network weights from the Q-network\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def get_action(self, state, training=True):\n",
    "        # Get the current state, determine if exploration or exploitation\n",
    "        if training and np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "\n",
    "        # Predict Q values\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(self.model(state)).item()\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "        # Convert to PyTorch tensors for quicker computation\n",
    "        states = torch.FloatTensor(states)                       # Shape: [batch, state_dim]\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1)         # Shape: [batch, 1]\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)        # Shape: [batch, 1]\n",
    "        next_states = torch.FloatTensor(next_states)             # Shape: [batch, state_dim]\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)            # Shape: [batch, 1]\n",
    "\n",
    "        # Predict Q(s,a) using current model\n",
    "        q_values = self.model(states).gather(1, actions)\n",
    "\n",
    "        # Predict max Q(s',a') for next state using target model\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "\n",
    "        # Compute target Q-values\n",
    "        targets = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.criterion(q_values, targets)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "def train_dqn(env, agent, episodes=500, max_steps=100, log_interval=100):\n",
    "    rewards = []\n",
    "    steps_list = []\n",
    "    start_time = time.time()\n",
    "    total_reward_accumulated = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = np.array(env.reset())\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = agent.get_action(state)                         # Get action\n",
    "            next_state, reward, done, _ = env.step(action)           # Take step\n",
    "            #reward = np.clip(reward, -1, 1)                          # clip reward\n",
    "            next_state = np.array(next_state)\n",
    "            agent.update(state, action, reward, next_state, done)    # Update state\n",
    "\n",
    "            # Update state and total reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        agent.replay()\n",
    "        rewards.append(total_reward)\n",
    "        total_reward_accumulated += total_reward\n",
    "        steps_list.append(step + 1)\n",
    "\n",
    "        # Decay exploration rate\n",
    "        if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon *= agent.epsilon_decay\n",
    "\n",
    "        # Update target network every 10 episodes\n",
    "        if episode % 10 == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        if episode % log_interval == 0:\n",
    "            avg_reward = np.mean(rewards[-log_interval:])\n",
    "            avg_steps = np.mean(steps_list[-log_interval:])\n",
    "            print(f\"Episode {episode}/{episodes} | Avg Reward: {avg_reward:.2f} | Avg Steps: {avg_steps:.2f} | Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "\n",
    "    print(f\"\\nTraining completed in {time.time() - start_time:.2f} seconds.\")\n",
    "    print(f\"\\nTotal Reward Accumulated: {total_reward_accumulated}.\")\n",
    "    return rewards\n",
    "\n",
    "def evaluate_dqn(env, agent, episodes=10, max_steps=100):\n",
    "    rewards = []\n",
    "    success_count = 0\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state = np.array(env.reset())\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = agent.get_action(state, training=False)  # Use greedy policy\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = np.array(next_state)\n",
    "\n",
    "            if done:\n",
    "                if reward > 0:  # Assuming reaching goal gives +10\n",
    "                    success_count += 1\n",
    "                break\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {ep + 1}/{episodes} | Reward: {total_reward:.2f} | Steps: {step + 1}\")\n",
    "\n",
    "    avg_reward = np.mean(rewards)\n",
    "    success_rate = success_count / episodes\n",
    "\n",
    "    print(f\"\\nEvaluation over {episodes} episodes - Average Reward: {avg_reward:.2f} | Success Rate: {success_rate * 100:.2f}%\")\n",
    "\n",
    "    return rewards\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def plot_reward_rate(rewards):\n",
    "    # Plot Average Reward per Episode\n",
    "    '''plt.plot(rewards)\n",
    "    plt.title('Average Reward per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward')'''\n",
    "\n",
    "    window = 100\n",
    "    moving_avg = [np.mean(rewards[i:i+window]) for i in range(len(rewards)-window)]\n",
    "    plt.plot(moving_avg)\n",
    "    plt.title('Moving Average Reward (Per 100 Episodes)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xeLrxTY4CS5R",
    "outputId": "3bce3c20-9843-4c58-fe5c-5a6dd54f793e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A' 0 0 0 0]\n",
      " [0 0 1 0 1]\n",
      " [0 0 0 1 1]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 'G']]\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(grid_size=(5,5), prob_obstacle=0.2)\n",
    "env.display_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "FlrW8Y5ICfh9",
    "outputId": "f2471e63-4009-4760-c63d-b2442a42dd8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50/500 | Avg Reward: 6.56 | Avg Steps: 24.92 | QΔ: 10.2683 | Epsilon: 0.9512\n",
      "Episode 100/500 | Avg Reward: 67.60 | Avg Steps: 24.26 | QΔ: 17.8702 | Epsilon: 0.9048\n",
      "Episode 150/500 | Avg Reward: 210.28 | Avg Steps: 23.40 | QΔ: 71.4253 | Epsilon: 0.8606\n",
      "Episode 200/500 | Avg Reward: 272.68 | Avg Steps: 22.82 | QΔ: 192.2578 | Epsilon: 0.8186\n",
      "Episode 250/500 | Avg Reward: 313.96 | Avg Steps: 22.54 | QΔ: 302.6838 | Epsilon: 0.7787\n",
      "Episode 300/500 | Avg Reward: 455.38 | Avg Steps: 21.34 | QΔ: 257.5810 | Epsilon: 0.7407\n",
      "Episode 350/500 | Avg Reward: 617.02 | Avg Steps: 19.70 | QΔ: 181.1049 | Epsilon: 0.7046\n",
      "Episode 400/500 | Avg Reward: 394.80 | Avg Steps: 21.54 | QΔ: 120.2530 | Epsilon: 0.6702\n",
      "Episode 450/500 | Avg Reward: 616.18 | Avg Steps: 20.14 | QΔ: 62.8999 | Epsilon: 0.6375\n",
      "Episode 500/500 | Avg Reward: 717.70 | Avg Steps: 19.36 | QΔ: 29.3507 | Epsilon: 0.6064\n",
      "\n",
      "Training completed in 0.40 seconds.\n",
      "\n",
      "Total Reward Accumulated: 183608.\n",
      "\n",
      "Episode 0/5000 | Avg Reward: -62.00 | Avg Steps: 50.00 | Epsilon: 0.9990\n",
      "Episode 500/5000 | Avg Reward: 207.59 | Avg Steps: 44.08 | Epsilon: 0.6058\n",
      "Episode 1000/5000 | Avg Reward: 127.97 | Avg Steps: 44.82 | Epsilon: 0.3673\n",
      "Episode 1500/5000 | Avg Reward: 845.66 | Avg Steps: 22.27 | Epsilon: 0.2227\n",
      "Episode 2000/5000 | Avg Reward: 659.55 | Avg Steps: 24.69 | Epsilon: 0.1351\n",
      "Episode 2500/5000 | Avg Reward: 303.94 | Avg Steps: 35.86 | Epsilon: 0.0819\n",
      "Episode 3000/5000 | Avg Reward: 423.15 | Avg Steps: 31.30 | Epsilon: 0.0497\n",
      "Episode 3500/5000 | Avg Reward: 390.39 | Avg Steps: 32.72 | Epsilon: 0.0301\n",
      "Episode 4000/5000 | Avg Reward: 376.32 | Avg Steps: 32.68 | Epsilon: 0.0183\n",
      "Episode 4500/5000 | Avg Reward: 583.58 | Avg Steps: 24.75 | Epsilon: 0.0111\n",
      "\n",
      "Training completed in 65.40 seconds.\n",
      "\n",
      "Total Reward Accumulated: 2377959.\n"
     ]
    }
   ],
   "source": [
    "# Use the same hyperparameters to test the modles\n",
    "\n",
    "q_agent = QLearningAgent(grid_size=(5,5), alpha=.1, gamma=.99, epsilon=1.0, epsilon_min=0.001, epsilon_decay=0.999)\n",
    "dqn_agent =DQNAgent(grid_size=(5,5), gamma=.99, epsilon=1.0, epsilon_min=0.001, epsilon_decay=0.999)\n",
    "\n",
    "rewards, q_changes = train(env, q_agent, episodes=500, max_steps=25, log_interval=50)\n",
    "r = train_dqn(env, dqn_agent, episodes=5000, max_steps=50, log_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v-XbmY0pfW4b",
    "outputId": "6cfdd08f-dc7d-422c-db7d-cf608ea35905"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/10 | Reward: 993 | Steps: 8\n",
      "Episode 2/10 | Reward: 993 | Steps: 8\n",
      "Episode 3/10 | Reward: 993 | Steps: 8\n",
      "Episode 4/10 | Reward: 993 | Steps: 8\n",
      "Episode 5/10 | Reward: 993 | Steps: 8\n",
      "Episode 6/10 | Reward: 993 | Steps: 8\n",
      "Episode 7/10 | Reward: 993 | Steps: 8\n",
      "Episode 8/10 | Reward: 993 | Steps: 8\n",
      "Episode 9/10 | Reward: 993 | Steps: 8\n",
      "Episode 10/10 | Reward: 993 | Steps: 8\n",
      "\n",
      "Success Rate: 100.00%\n",
      "\n",
      "Episode 1/10 | Reward: 977.00 | Steps: 24\n",
      "Episode 2/10 | Reward: 980.00 | Steps: 21\n",
      "Episode 3/10 | Reward: 964.00 | Steps: 37\n",
      "Episode 4/10 | Reward: 971.00 | Steps: 30\n",
      "Episode 5/10 | Reward: 958.00 | Steps: 43\n",
      "Episode 6/10 | Reward: 955.00 | Steps: 46\n",
      "Episode 7/10 | Reward: 984.00 | Steps: 17\n",
      "Episode 8/10 | Reward: 959.00 | Steps: 42\n",
      "Episode 9/10 | Reward: 957.00 | Steps: 44\n",
      "Episode 10/10 | Reward: 982.00 | Steps: 19\n",
      "\n",
      "Evaluation over 10 episodes - Average Reward: 968.70 | Success Rate: 100.00%\n"
     ]
    }
   ],
   "source": [
    "ev_reward = evaluate(env, q_agent, episodes=10)\n",
    "r = evaluate_dqn(env, dqn_agent, max_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AiyhkqCSnThJ",
    "outputId": "096a68a4-eaef-44b3-e2a4-5f3de65563ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A' 0 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 1 0 1 0]\n",
      " [0 1 0 0 0 1 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 1 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 1 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 0 0 1 0 0 1]\n",
      " [1 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 1 0 0 0 1 0 0 'G']]\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(grid_size=(15,15), prob_obstacle=0.2)\n",
    "env.display_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "L2kXscZ6noXQ",
    "outputId": "40ee2d77-cafd-4f0e-8c84-8a8931039c0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/1000 | Avg Reward: -333.17 | Avg Steps: 246.61 | QΔ: 25.6496 | Epsilon: 0.9048\n",
      "Episode 200/1000 | Avg Reward: 83.06 | Avg Steps: 214.66 | QΔ: 179.8769 | Epsilon: 0.8186\n",
      "Episode 300/1000 | Avg Reward: 592.84 | Avg Steps: 160.78 | QΔ: 644.5393 | Epsilon: 0.7407\n",
      "Episode 400/1000 | Avg Reward: 837.34 | Avg Steps: 112.97 | QΔ: 569.3860 | Epsilon: 0.6702\n",
      "Episode 500/1000 | Avg Reward: 896.65 | Avg Steps: 80.75 | QΔ: 405.3540 | Epsilon: 0.6064\n",
      "Episode 600/1000 | Avg Reward: 908.81 | Avg Steps: 74.11 | QΔ: 350.3008 | Epsilon: 0.5486\n",
      "Episode 700/1000 | Avg Reward: 928.94 | Avg Steps: 61.34 | QΔ: 200.5112 | Epsilon: 0.4964\n",
      "Episode 800/1000 | Avg Reward: 936.77 | Avg Steps: 55.39 | QΔ: 87.8012 | Epsilon: 0.4491\n",
      "Episode 900/1000 | Avg Reward: 938.77 | Avg Steps: 53.71 | QΔ: 49.3134 | Epsilon: 0.4064\n",
      "Episode 1000/1000 | Avg Reward: 945.42 | Avg Steps: 48.70 | QΔ: 22.3255 | Epsilon: 0.3677\n",
      "\n",
      "Training completed in 3.92 seconds.\n",
      "\n",
      "Total Reward Accumulated: 673543.\n",
      "\n",
      "Episode 0/3000 | Avg Reward: -402.00 | Avg Steps: 250.00 | Epsilon: 0.9990\n",
      "Episode 200/3000 | Avg Reward: -357.00 | Avg Steps: 247.09 | Epsilon: 0.8178\n",
      "Episode 400/3000 | Avg Reward: -154.71 | Avg Steps: 231.03 | Epsilon: 0.6695\n",
      "Episode 600/3000 | Avg Reward: 358.68 | Avg Steps: 178.81 | Epsilon: 0.5481\n",
      "Episode 800/3000 | Avg Reward: 692.91 | Avg Steps: 121.92 | Epsilon: 0.4487\n",
      "Episode 1000/3000 | Avg Reward: 499.07 | Avg Steps: 157.81 | Epsilon: 0.3673\n",
      "Episode 1200/3000 | Avg Reward: 768.93 | Avg Steps: 101.76 | Epsilon: 0.3007\n",
      "Episode 1400/3000 | Avg Reward: 721.78 | Avg Steps: 93.10 | Epsilon: 0.2462\n",
      "Episode 1600/3000 | Avg Reward: 717.50 | Avg Steps: 88.34 | Epsilon: 0.2015\n",
      "Episode 1800/3000 | Avg Reward: 763.75 | Avg Steps: 81.98 | Epsilon: 0.1650\n",
      "Episode 2000/3000 | Avg Reward: 721.67 | Avg Steps: 87.21 | Epsilon: 0.1351\n",
      "Episode 2200/3000 | Avg Reward: 562.74 | Avg Steps: 108.56 | Epsilon: 0.1106\n",
      "Episode 2400/3000 | Avg Reward: 485.71 | Avg Steps: 115.86 | Epsilon: 0.0905\n",
      "Episode 2600/3000 | Avg Reward: 441.11 | Avg Steps: 125.47 | Epsilon: 0.0741\n",
      "Episode 2800/3000 | Avg Reward: 454.75 | Avg Steps: 122.68 | Epsilon: 0.0607\n",
      "\n",
      "Training completed in 90.99 seconds.\n",
      "\n",
      "Total Reward Accumulated: 1371605.\n"
     ]
    }
   ],
   "source": [
    "q_agent = QLearningAgent(grid_size=(15,15), alpha=.1, gamma=.97, epsilon=1.0, epsilon_min=0.001, epsilon_decay=0.999)\n",
    "dqn_agent = DQNAgent(grid_size=(15,15), gamma=.98, epsilon=1.0, epsilon_min=0.001, epsilon_decay=0.999)\n",
    "\n",
    "rewards, q_changes = train(env, q_agent, episodes=1000, max_steps=250, log_interval=100)\n",
    "r = train_dqn(env, dqn_agent, episodes=3000, max_steps=250, log_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2b2vqx05mnD",
    "outputId": "967e5db4-89d1-41ba-e409-d650e8c5e682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/10 | Reward: 973 | Steps: 28\n",
      "Episode 2/10 | Reward: 973 | Steps: 28\n",
      "Episode 3/10 | Reward: 973 | Steps: 28\n",
      "Episode 4/10 | Reward: 973 | Steps: 28\n",
      "Episode 5/10 | Reward: 973 | Steps: 28\n",
      "Episode 6/10 | Reward: 973 | Steps: 28\n",
      "Episode 7/10 | Reward: 973 | Steps: 28\n",
      "Episode 8/10 | Reward: 973 | Steps: 28\n",
      "Episode 9/10 | Reward: 973 | Steps: 28\n",
      "Episode 10/10 | Reward: 973 | Steps: 28\n",
      "\n",
      "Success Rate: 100.00%\n",
      "\n",
      "Episode 1/10 | Reward: -4556.00 | Steps: 1500\n",
      "Episode 2/10 | Reward: 915.00 | Steps: 42\n",
      "Episode 3/10 | Reward: -1277.00 | Steps: 482\n",
      "Episode 4/10 | Reward: -5128.00 | Steps: 1500\n",
      "Episode 5/10 | Reward: 697.00 | Steps: 116\n",
      "Episode 6/10 | Reward: -4760.00 | Steps: 1181\n",
      "Episode 7/10 | Reward: 240.00 | Steps: 181\n",
      "Episode 8/10 | Reward: 934.00 | Steps: 43\n",
      "Episode 9/10 | Reward: -1260.00 | Steps: 481\n",
      "Episode 10/10 | Reward: 951.00 | Steps: 34\n",
      "\n",
      "Evaluation over 10 episodes - Average Reward: -1324.40 | Success Rate: 80.00%\n"
     ]
    }
   ],
   "source": [
    "ev_reward = evaluate(env, q_agent, episodes=10, max_steps=1250)\n",
    "r = evaluate_dqn(env, dqn_agent, max_steps=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xTFz60UHcqI_",
    "outputId": "7feddc48-0ad5-44b0-d6d5-862e74a21d91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A' 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 'G']]\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(grid_size=(30,30), prob_obstacle=0.1)\n",
    "env.display_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qCaHaSpNdAJY",
    "outputId": "d75cb160-9d9b-4bbf-d9bf-5f4a2ca480fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/2000 | Avg Reward: -640.96 | Avg Steps: 450.00 | QΔ: 39.8694 | Epsilon: 0.9048\n",
      "Episode 200/2000 | Avg Reward: -608.76 | Avg Steps: 450.00 | QΔ: 37.7453 | Epsilon: 0.8186\n",
      "Episode 300/2000 | Avg Reward: -481.41 | Avg Steps: 444.62 | QΔ: 48.3495 | Epsilon: 0.7407\n",
      "Episode 400/2000 | Avg Reward: -197.12 | Avg Steps: 417.24 | QΔ: 101.8138 | Epsilon: 0.6702\n",
      "Episode 500/2000 | Avg Reward: -4.61 | Avg Steps: 387.72 | QΔ: 186.6828 | Epsilon: 0.6064\n",
      "Episode 600/2000 | Avg Reward: 227.29 | Avg Steps: 353.68 | QΔ: 247.6442 | Epsilon: 0.5486\n",
      "Episode 700/2000 | Avg Reward: 435.74 | Avg Steps: 312.18 | QΔ: 302.4905 | Epsilon: 0.4964\n",
      "Episode 800/2000 | Avg Reward: 572.56 | Avg Steps: 267.36 | QΔ: 284.4716 | Epsilon: 0.4491\n",
      "Episode 900/2000 | Avg Reward: 703.85 | Avg Steps: 243.01 | QΔ: 299.3070 | Epsilon: 0.4064\n",
      "Episode 1000/2000 | Avg Reward: 762.89 | Avg Steps: 210.43 | QΔ: 274.7605 | Epsilon: 0.3677\n",
      "Episode 1100/2000 | Avg Reward: 784.67 | Avg Steps: 185.80 | QΔ: 239.2158 | Epsilon: 0.3327\n",
      "Episode 1200/2000 | Avg Reward: 823.98 | Avg Steps: 163.10 | QΔ: 225.4817 | Epsilon: 0.3010\n",
      "Episode 1300/2000 | Avg Reward: 840.31 | Avg Steps: 147.09 | QΔ: 195.0644 | Epsilon: 0.2724\n",
      "Episode 1400/2000 | Avg Reward: 855.71 | Avg Steps: 134.69 | QΔ: 176.6341 | Epsilon: 0.2464\n",
      "Episode 1500/2000 | Avg Reward: 874.34 | Avg Steps: 118.34 | QΔ: 169.7801 | Epsilon: 0.2230\n",
      "Episode 1600/2000 | Avg Reward: 886.30 | Avg Steps: 108.18 | QΔ: 135.8302 | Epsilon: 0.2017\n",
      "Episode 1700/2000 | Avg Reward: 900.41 | Avg Steps: 95.67 | QΔ: 126.5854 | Epsilon: 0.1825\n",
      "Episode 1800/2000 | Avg Reward: 908.47 | Avg Steps: 88.37 | QΔ: 117.7866 | Epsilon: 0.1652\n",
      "Episode 1900/2000 | Avg Reward: 918.03 | Avg Steps: 79.93 | QΔ: 115.7608 | Epsilon: 0.1494\n",
      "Episode 2000/2000 | Avg Reward: 926.88 | Avg Steps: 72.24 | QΔ: 96.3851 | Epsilon: 0.1352\n",
      "\n",
      "Training completed in 15.19 seconds.\n",
      "\n",
      "Total Reward Accumulated: 948857.\n",
      "\n",
      "Episode 0/1500 | Avg Reward: 68.00 | Avg Steps: 837.00 | Epsilon: 0.4995\n",
      "Episode 100/1500 | Avg Reward: 550.81 | Avg Steps: 323.74 | Epsilon: 0.4519\n",
      "Episode 200/1500 | Avg Reward: 383.70 | Avg Steps: 392.43 | Epsilon: 0.4089\n",
      "Episode 300/1500 | Avg Reward: 560.53 | Avg Steps: 257.56 | Epsilon: 0.3700\n",
      "Episode 400/1500 | Avg Reward: 728.40 | Avg Steps: 165.36 | Epsilon: 0.3348\n",
      "Episode 500/1500 | Avg Reward: 647.64 | Avg Steps: 195.52 | Epsilon: 0.3029\n",
      "Episode 600/1500 | Avg Reward: 679.08 | Avg Steps: 177.24 | Epsilon: 0.2740\n",
      "Episode 700/1500 | Avg Reward: 769.58 | Avg Steps: 140.70 | Epsilon: 0.2480\n",
      "Episode 800/1500 | Avg Reward: 730.85 | Avg Steps: 154.55 | Epsilon: 0.2243\n",
      "Episode 900/1500 | Avg Reward: -402.14 | Avg Steps: 625.70 | Epsilon: 0.2030\n",
      "Episode 1000/1500 | Avg Reward: -53.46 | Avg Steps: 536.33 | Epsilon: 0.1837\n",
      "Episode 1100/1500 | Avg Reward: 624.31 | Avg Steps: 228.85 | Epsilon: 0.1662\n",
      "Episode 1200/1500 | Avg Reward: 770.42 | Avg Steps: 151.86 | Epsilon: 0.1504\n",
      "Episode 1300/1500 | Avg Reward: 756.72 | Avg Steps: 141.24 | Epsilon: 0.1360\n",
      "Episode 1400/1500 | Avg Reward: 508.47 | Avg Steps: 249.49 | Epsilon: 0.1231\n",
      "\n",
      "Training completed in 116.85 seconds.\n",
      "\n",
      "Total Reward Accumulated: 774593.\n"
     ]
    }
   ],
   "source": [
    "# Use the same hyperparameters to test the modles\n",
    "\n",
    "q_agent = QLearningAgent(grid_size=(30,30), alpha=.1, gamma=.97, epsilon=1.0, epsilon_min=0.001, epsilon_decay=0.999)\n",
    "dqn_agent = DQNAgent(grid_size=(30,30), gamma=.97, epsilon=0.5, epsilon_min=0.001, epsilon_decay=0.999)\n",
    "\n",
    "rewards, q_changes = train(env, q_agent, episodes=2000, max_steps=450, log_interval=100)\n",
    "rewards = train_dqn(env, dqn_agent, episodes=1500, max_steps=900, log_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fzuvB3K7d3bP",
    "outputId": "72d610a5-0cca-43f6-c140-e75fd128f5a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/10 | Reward: 943 | Steps: 58\n",
      "Episode 2/10 | Reward: 943 | Steps: 58\n",
      "Episode 3/10 | Reward: 943 | Steps: 58\n",
      "Episode 4/10 | Reward: 943 | Steps: 58\n",
      "Episode 5/10 | Reward: 943 | Steps: 58\n",
      "Episode 6/10 | Reward: 943 | Steps: 58\n",
      "Episode 7/10 | Reward: 943 | Steps: 58\n",
      "Episode 8/10 | Reward: 943 | Steps: 58\n",
      "Episode 9/10 | Reward: 943 | Steps: 58\n",
      "Episode 10/10 | Reward: 943 | Steps: 58\n",
      "\n",
      "Success Rate: 100.00%\n",
      "\n",
      "Episode 1/10 | Reward: -1452.00 | Steps: 500\n",
      "Episode 2/10 | Reward: 774.00 | Steps: 107\n",
      "Episode 3/10 | Reward: 680.00 | Steps: 129\n",
      "Episode 4/10 | Reward: 884.00 | Steps: 89\n",
      "Episode 5/10 | Reward: 800.00 | Steps: 101\n",
      "Episode 6/10 | Reward: 877.00 | Steps: 84\n",
      "Episode 7/10 | Reward: 757.00 | Steps: 116\n",
      "Episode 8/10 | Reward: 194.00 | Steps: 315\n",
      "Episode 9/10 | Reward: 811.00 | Steps: 102\n",
      "Episode 10/10 | Reward: 812.00 | Steps: 93\n",
      "\n",
      "Evaluation over 10 episodes - Average Reward: 513.70 | Success Rate: 90.00%\n"
     ]
    }
   ],
   "source": [
    "ev_reward = evaluate(env, q_agent, episodes=10, max_steps=500)\n",
    "r = evaluate_dqn(env, dqn_agent, max_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
